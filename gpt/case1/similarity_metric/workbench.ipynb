{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data_merged_updated_similarity_few_shot.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_merged_updated_similarity_few_shot.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa = df[(df['source'] == 'GAS') | (df['source'] == 'WVS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['source'] == 'GPT-4 generated') |(data['source'] == 'GPT / https://www.beelovedcity.com/england-quiz')\n",
    "            |(data['source'] == 'GPT / https://www.anadventurousworld.com/usa-trivia-questions/')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def gpt4_text_similarity(text1, text2, model=\"gpt-4-1106-preview\"):\n",
    "    \"\"\"\n",
    "    Measures the similarity between two texts using GPT-4.\n",
    "\n",
    "    Parameters:\n",
    "    text1 (str): First text for comparison.\n",
    "    text2 (str): Second text for comparison.\n",
    "    model (str): The GPT model to use.\n",
    "\n",
    "    Returns:\n",
    "    float: A similarity score between 0 (not similar) and 1 (very similar).\n",
    "    \"\"\"\n",
    "\n",
    "    system = f\"On a scale between 1 and 5, how similar are the following two sentences? Respond only with a score between 1 and 5.\"\n",
    "    prompt = f\"\"\"\n",
    "                Examples:\n",
    "                Input: \n",
    "                Sentence 1: Not really, too busy for that.\n",
    "                Sentence 2: Yes, quite fond of academic journals.\n",
    "                Output: 1.0\n",
    "                Input: \n",
    "                Sentence 1: Universal healthcare, accessible to all residents.\n",
    "                Sentence 2: NHS provides universal healthcare for all.\n",
    "                Output: 2.0\n",
    "                Input: \n",
    "                Sentence 1: Based on recommendations and personal interests.\n",
    "                Sentence 2: By author, genre, recommendations, and reviews.\n",
    "                Output: 3.0\n",
    "                Input: \n",
    "                Sentence 1: As often as I can.\n",
    "                Sentence 2: Quite often, I'm always willing.\n",
    "                Output: 4.0\n",
    "                Input: \n",
    "                Sentence 1: Yes, I love wearing hats!\n",
    "                Sentence 2: Yes, I quite fancy wearing hats.\n",
    "                Output: 5.0    \n",
    "                \\nInput:\\n\\nSentence 1: {text1}\\n\\n Sentence 2: {text2}. \\n Output:\"\"\"\n",
    "    #prompt = f\"Rate the similarity between the following two texts on a scale from 0 (completely different) to 1 (identical):\\n\\nText 1: {text1}\\n\\nText 2: {text2}\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  prompt\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=20\n",
    "    )\n",
    "    #print(response.choices[0].message.content)\n",
    "    # Extracting the similarity score from the response\n",
    "    try:\n",
    "        last_message = response.choices[0].message.content\n",
    "        similarity_score = float(last_message.strip())\n",
    "    except (ValueError, KeyError, IndexError):\n",
    "        similarity_score = None\n",
    "    print(similarity_score)\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Normalize the values in a DataFrame column to the range 0-1 and save them in a new column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column to normalize.\n",
    "    column_name (str): The name of the column to normalize.\n",
    "    new_column_name (str): The name of the new column for the normalized values.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the additional normalized column.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original data\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # Apply Min-Max normalization\n",
    "    df_normalized[new_column_name] = (df_normalized[column_name] - 1) / (5 - 1)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_small_value(df, column_name):\n",
    "    \"\"\"\n",
    "    Add 0.00001 to each value in the specified column of a DataFrame \n",
    "    if the value is not 0.0 or 1.0.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column.\n",
    "    column_name (str): The name of the column to modify.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the modified column.\n",
    "    \"\"\"\n",
    "    # Define the lambda function for the condition\n",
    "    add_value = lambda x: x + 0.000001 if x not in [0.0, 1.0] else x\n",
    "\n",
    "    # Apply the function to the specified column\n",
    "    df[column_name] = df[column_name].apply(add_value)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uk_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_uk'], row['answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['uk_score'] = data.apply(apply_similarity, axis=1)\n",
    "print(\"uk_score Finished\")\n",
    "\n",
    "print(\"us_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['us_score'] = data.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"us_score finished\")\n",
    "\n",
    "print(\"ukGT_usGT_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['ukGT_usGT_score'] = data.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"ukGT_usGT_score finished\")\n",
    "\n",
    "print(\"ukGT_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['model_answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['ukGT_usMA_score'] = data.apply(apply_similarity, axis=1)\n",
    "print(\"ukGT_usMA_score finished\")\n",
    "\n",
    "print(\"ukMA_usGT_score start\")\n",
    "\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['ukMA_usGT_score'] = data.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usGT_score finished\")\n",
    "\n",
    "print(\"ukMA_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "data['ukMA_usMA_score'] = data.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usMA_score finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'selections', 'options', 'options_formatted',\n",
       "       'source', 'value_us', 'value_uk', 'answer_us', 'answer_uk', 'category',\n",
       "       '# of options', 'question type', 'category_group', 'model_answer_us',\n",
       "       'model_answer_uk', 'model_answer_uk_option_match',\n",
       "       'model_answer_us_option_match', '#_options', 'options_dict',\n",
       "       'score_ground_truth_answers', 'value_diff', 'question_type',\n",
       "       'ukGT_usGT_score', 'ukMA_usGT_score', 'ukGT_usMA_score',\n",
       "       'ukMA_usMA_score', 'us_score', 'uk_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_column(data, 'us_score', 'us_score')\n",
    "data = normalize_column(data, 'uk_score', 'uk_score')\n",
    "data = normalize_column(data, 'ukGT_usGT_score', 'ukGT_usGT_score')\n",
    "data = normalize_column(data, 'ukMA_usGT_score', 'ukMA_usGT_score')\n",
    "data = normalize_column(data, 'ukGT_usMA_score', 'ukGT_usMA_score')\n",
    "data = normalize_column(data, 'ukMA_usMA_score', 'ukMA_usMA_score')\n",
    "data = add_small_value(data, 'us_score')\n",
    "data = add_small_value(data, 'uk_score')\n",
    "data = add_small_value(data, 'ukGT_usGT_score')\n",
    "data = add_small_value(data, 'ukMA_usGT_score')\n",
    "data = add_small_value(data, 'ukGT_usMA_score')\n",
    "data = add_small_value(data, 'ukMA_usMA_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([goqa, data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "US-Score: 0.5751162591209428\n",
      "UK-Score: 0.5612361412937029\n",
      "GroundTruth-Score: 0.4301639327324609\n",
      "UK-ModelAnswer_vs_US-GroundTruth-Score: 0.46900712058759736\n",
      "US-ModelAnswer_vs_UK-GroundTruth-Score: 0.4636023074677981\n",
      "UK-ModelAnswer_vs_US-ModelAnswer-Score: 0.7145111496862704\n",
      "Data size: 1015\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "inspect = merged_df[merged_df['ukGT_usGT_score'] < 0.9]\n",
    "print(\"-----------------\")\n",
    "print(\"US-Score:\", inspect.us_score.mean())\n",
    "print(\"UK-Score:\", inspect.uk_score.mean())\n",
    "print(\"GroundTruth-Score:\", inspect.ukGT_usGT_score.mean())\n",
    "print(\"UK-ModelAnswer_vs_US-GroundTruth-Score:\", inspect.ukMA_usGT_score.mean())\n",
    "print(\"US-ModelAnswer_vs_UK-GroundTruth-Score:\", inspect.ukGT_usMA_score.mean())\n",
    "print(\"UK-ModelAnswer_vs_US-ModelAnswer-Score:\", inspect.ukMA_usMA_score.mean())\n",
    "print(\"Data size:\", len(inspect))\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
