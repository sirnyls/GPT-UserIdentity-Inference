{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/data_merged_with_scores.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "GAS                                                               303\n",
       "GPT / https://www.anadventurousworld.com/usa-trivia-questions/     14\n",
       "GPT / https://www.beelovedcity.com/england-quiz                    33\n",
       "GPT-4 generated                                                   567\n",
       "WVS                                                                84\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'].value_counts().sort_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_group\n",
       "Economy, Health, and Environment              178\n",
       "Lifestyle, Entertainment, and Daily Living    285\n",
       "Media, Technology, and Education              141\n",
       "Politics and Governance                       206\n",
       "Society, Culture, and Relationships           191\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_group'].value_counts().sort_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question type\n",
       "Binary Choice      475\n",
       "Likert Scale       292\n",
       "Multiple Choice    131\n",
       "Numerical Scale     18\n",
       "Ordinal Scale       38\n",
       "Quiz question       47\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question type'].value_counts().sort_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity GPT-US: 0.7404109597490045 \n",
      "Similarity GPT-UK: 0.7113406496108827 \n",
      "Similarity UK-US: 28.43867951781048\n",
      "Number of q,a,a triples: 1002\n"
     ]
    }
   ],
   "source": [
    "mean_score_ground_truths = df['similarity_ground_truth_answers_uk_us'].mean()\n",
    "mean_score_us = df['overall_score_us'].mean()\n",
    "mean_score_uk = df['overall_score_uk'].mean()\n",
    "print(\"Similarity GPT-US: \" + str(mean_score_us), \n",
    "      \"\\nSimilarity GPT-UK: \" + str(mean_score_uk), \n",
    "      \"\\nSimilarity UK-US: \" + str(mean_score_ground_truths))\n",
    "print(\"Number of q,a,a triples: \" + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_word_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nils/GPT-UserIdentity-Inference/gpt/case1/statistics/statistics_dataset.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nils/GPT-UserIdentity-Inference/gpt/case1/statistics/statistics_dataset.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvg. number of words per question: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_word_count(df, \u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nils/GPT-UserIdentity-Inference/gpt/case1/statistics/statistics_dataset.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvg. number of words per answer: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m((average_word_count(df, \u001b[39m'\u001b[39m\u001b[39manswer_us\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m average_word_count(df, \u001b[39m'\u001b[39m\u001b[39manswer_uk\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nils/GPT-UserIdentity-Inference/gpt/case1/statistics/statistics_dataset.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnique words dataset: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(unique_word_count(df, \u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average_word_count' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Avg. number of words per question: \" + str(average_word_count(df, 'question')))\n",
    "print(\"Avg. number of words per answer: \" + str((average_word_count(df, 'answer_us') + average_word_count(df, 'answer_uk')) / 2))\n",
    "print(\"Unique words dataset: \" + str(unique_word_count(df, 'question')))\n",
    "\n",
    "print(\"GOQA - Avg. number of words per question: \" + str(average_word_count(goqa, 'question')))\n",
    "print(\"GOQA - Avg. number of words per answer: \" + str((average_word_count(goqa, 'answer_us') + average_word_count(goqa, 'answer_uk')) / 2))\n",
    "print(\"GOQA - Unique words dataset: \" + str(unique_word_count(goqa, 'question')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_count(df, column_name):\n",
    "    # Convert the column to strings\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    # Calculate the number of words for each row in the column\n",
    "    word_counts = df[column_name].apply(lambda x: len(x.split()))\n",
    "    # Return the average word count\n",
    "    return word_counts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_count(df, column_name):\n",
    "\n",
    "    # Convert the column to strings\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    # Collect all words from each row in the column\n",
    "    all_words = df[column_name].apply(lambda x: x.split()).explode()\n",
    "\n",
    "    # Count unique words\n",
    "    unique_words = set(all_words)\n",
    "\n",
    "    return len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_values(value):\n",
    "    # Check if value is an integer or greater than 1\n",
    "    if value > 1 or isinstance(value, int):\n",
    "        return value / 1000\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Apply the correction function to the column\n",
    "df['similarity_ground_truth_answers_uk_us'] = df['similarity_ground_truth_answers_uk_us'].apply(correct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity GPT-US: 0.7832165178588248 \n",
      "Similarity GPT-UK: 0.7346302110576967 \n",
      "Similarity UK-US: 0.6852913584027972\n",
      "Number of q,a,a triples: 567\n"
     ]
    }
   ],
   "source": [
    "gpt_generated = df[df['source'] == 'GPT-4 generated']\n",
    "mean_score_ground_truths = gpt_generated['similarity_ground_truth_answers_uk_us'].mean()\n",
    "mean_score_us = gpt_generated['overall_score_us'].mean()\n",
    "mean_score_uk = quiz['overall_score_uk'].mean()\n",
    "print(\"Similarity GPT-US: \" + str(mean_score_us), \n",
    "      \"\\nSimilarity GPT-UK: \" + str(mean_score_uk), \n",
    "      \"\\nSimilarity UK-US: \" + str(mean_score_ground_truths))\n",
    "print(\"Number of q,a,a triples: \" + str(len(quiz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity GPT-US: 0.6705235090639714 \n",
      "Similarity GPT-UK: 0.6599592853051901 \n",
      "Similarity UK-US: 0.6308118451079614\n",
      "Number of q,a,a triples: 387\n"
     ]
    }
   ],
   "source": [
    "goqa = df[(df['source'] == 'GAS') | (df['source'] == 'WVS')]\n",
    "mean_score_ground_truths = goqa['similarity_ground_truth_answers_uk_us'].mean()\n",
    "mean_score_us = goqa['overall_score_us'].mean()\n",
    "mean_score_uk = goqa['overall_score_uk'].mean()\n",
    "print(\"Similarity GPT-US: \" + str(mean_score_us), \n",
    "      \"\\nSimilarity GPT-UK: \" + str(mean_score_uk), \n",
    "      \"\\nSimilarity UK-US: \" + str(mean_score_ground_truths))\n",
    "print(\"Number of q,a,a triples: \" + str(len(goqa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity GPT-US: 0.8017642517039116 \n",
      "Similarity GPT-UK: 0.8564725924045482 \n",
      "Similarity UK-US: 0.7081266597230383\n",
      "Number of q,a,a triples: 47\n"
     ]
    }
   ],
   "source": [
    "quiz = df[(df['source'] == 'GPT / https://www.beelovedcity.com/england-quiz') | (df['source'] == 'GPT / https://www.anadventurousworld.com/usa-trivia-questions/')]\n",
    "mean_score_ground_truths = quiz['similarity_ground_truth_answers_uk_us'].mean()\n",
    "mean_score_us = quiz['overall_score_us'].mean()\n",
    "mean_score_uk = quiz['overall_score_uk'].mean()\n",
    "print(\"Similarity GPT-US: \" + str(mean_score_us), \n",
    "      \"\\nSimilarity GPT-UK: \" + str(mean_score_uk), \n",
    "      \"\\nSimilarity UK-US: \" + str(mean_score_ground_truths))\n",
    "print(\"Number of q,a,a triples: \" + str(len(quiz)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def gpt4_text_similarity(text1, text2, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Measures the similarity between two texts using GPT-4.\n",
    "\n",
    "    Parameters:\n",
    "    text1 (str): First text for comparison.\n",
    "    text2 (str): Second text for comparison.\n",
    "    model (str): The GPT model to use.\n",
    "\n",
    "    Returns:\n",
    "    float: A similarity score between 0 (not similar) and 1 (very similar).\n",
    "    \"\"\"\n",
    "\n",
    "    system = f\"On a scale between 1 and 5, how similar are the following two sentences? Respond only with a score between 1 and 5.\"\n",
    "    prompt = f\"Input:\\n\\n Sentence 1: {text1}\\n\\n Sentence 2: {text2}. \\n Output:\"\n",
    "    #prompt = f\"Rate the similarity between the following two texts on a scale from 0 (completely different) to 1 (identical):\\n\\nText 1: {text1}\\n\\nText 2: {text2}\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  prompt\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=20\n",
    "    )\n",
    "    #print(response.choices[0].message.content)\n",
    "    # Extracting the similarity score from the response\n",
    "    try:\n",
    "        last_message = response.choices[0].message.content\n",
    "        similarity_score = float(last_message.strip())\n",
    "    except (ValueError, KeyError, IndexError):\n",
    "        similarity_score = None\n",
    "    print(similarity_score)\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "4.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "4.5\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#gpt_generated = data[data['source'] == 'GPT-4 generated']\n",
    "quiz = pd.read_csv('quiz_answers_new.csv', sep=';')\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_uk'], row['answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_uk_score'] = quiz.apply(apply_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score Start\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "4.5\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "US-Score finished\n",
      "gpt4_usGT_ukGT_score start\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "4.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "4.0\n",
      "4.5\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "3.0\n",
      "5.0\n",
      "4.5\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "gpt4_usGT_ukGT_score finished\n",
      "gpt4_ukGT_usMA_score start\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "3.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.5\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "4.5\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "1.0\n",
      "gpt4_ukGT_usMA_score finished\n",
      "gpt4_usGT_ukMA_score start\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "3.5\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.5\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "3.5\n",
      "1.0\n",
      "5.0\n",
      "3.0\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "2.5\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "3.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "gpt4_usGT_ukMA_score finished\n",
      "gpt4_usMA_ukMA_score start\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "1.0\n",
      "2.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.0\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "5.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "4.5\n",
      "4.5\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "4.0\n",
      "4.5\n",
      "1.0\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "3.5\n",
      "5.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "5.0\n",
      "2.0\n",
      "1.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "gpt4_usMA_ukMA_score finished\n"
     ]
    }
   ],
   "source": [
    "print(\"US-Score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_us_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"US-Score finished\")\n",
    "\n",
    "print(\"gpt4_usGT_ukGT_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_usGT_ukGT_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"gpt4_usGT_ukGT_score finished\")\n",
    "\n",
    "print(\"gpt4_ukGT_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['model_answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_ukGT_usMA_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"gpt4_ukGT_usMA_score finished\")\n",
    "\n",
    "print(\"gpt4_usGT_ukMA_score start\")\n",
    "\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_usGT_ukMA_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"gpt4_usGT_ukMA_score finished\")\n",
    "\n",
    "print(\"gpt4_usMA_ukMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['gpt4_usMA_ukMA_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"gpt4_usMA_ukMA_score finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7081266597230383\n",
      "0.6085106382978724\n"
     ]
    }
   ],
   "source": [
    "# Ground Truths\n",
    "print(quiz['similarity_ground_truth_answers_uk_us'].mean())\n",
    "print(quiz['gpt-4_score_ground_truths'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7832165178588248\n",
      "0.6611640211640212\n"
     ]
    }
   ],
   "source": [
    "# US\n",
    "print(gpt_generated['overall_score_us'].mean())\n",
    "print(gpt_generated['gpt-4_us_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6852913584027972\n",
      "0.5305996472663139\n"
     ]
    }
   ],
   "source": [
    "# Ground Truths\n",
    "print(gpt_generated['similarity_ground_truth_answers_uk_us'].mean())\n",
    "print(gpt_generated['gpt-4_score_ground_truths'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7346302110576967\n",
      "0.6012345679012345\n"
     ]
    }
   ],
   "source": [
    "# UK Score\n",
    "print(gpt_generated['overall_score_uk'].mean())\n",
    "print(gpt_generated['gpt-4_uk_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_significantly_different(result1: list, result2: list, P_VALUE_THRES=0.05):\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    score, p_value = stats.ttest_ind(result1, np.array(result2), equal_var=False)\n",
    "    if_sign = p_value <= P_VALUE_THRES\n",
    "    return if_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Us_similarity_scores_per_sample = gpt_generated['gpt-4_us_score'].tolist()\n",
    "Uk_similarity_scores_per_sample = gpt_generated['gpt-4_uk_score'].tolist()\n",
    "if_significantly_different(Us_similarity_scores_per_sample,Uk_similarity_scores_per_sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_gpt_scores = gpt_generated[['answer_uk', 'answer_us', 'model_answer_uk', 'model_answer_us', 'gpt-4_uk_score', 'gpt-4_us_score', 'gpt-4_score_ground_truths']]\n",
    "eval_gpt_scores.to_csv('eval_gpt_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-categorize question types in GOQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def question_type(question, options_formatted, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Measures the similarity between two texts using GPT-4.\n",
    "\n",
    "    Parameters:\n",
    "    text1 (str): First text for comparison.\n",
    "    text2 (str): Second text for comparison.\n",
    "    model (str): The GPT model to use.\n",
    "\n",
    "    Returns:\n",
    "    float: A similarity score between 0 (not similar) and 1 (very similar).\n",
    "    \"\"\"\n",
    "\n",
    "    system = \"\"\"Categorize the following question into one of the following categories. \n",
    "                First, decide if it is a scale, free-text or multiple choice question. \n",
    "                If it is a scale type question, determine if it is ordinal scale, likert scale or numerical scale. \n",
    "                For multiple choice, determine wether it is a binary choice or multiple choice question.\n",
    "                Some questions have answer choices, some not.\n",
    "                Examples: \n",
    "                Input: \n",
    "                question: Which statement comes closer to your own views, even if neither is exactly right? Using overwhelming military force is the best way to defeat terrorism around the world, Relying too much on military force to defeat terrorism creates hatred that leads to more terrorism\n",
    "                options: \n",
    "                (A) Using overwhelming military force is the best way to defeat terrorism around the world\n",
    "                (B) If (survey country) had cooperated more with other countries, the number of coronavirus cases would have been lower in this country\n",
    "                (C) Many of the problems facing our country can be solved by working with other countries\n",
    "                (D) Relying too much on military force to defeat terrorism creates hatred that leads to more terrorism\n",
    "                (E) No amount of cooperation between (survey country) and other countries would have reduced the number of coronavirus cases in this country\n",
    "                (F) Few of the problems facing our country can be solved by working with other countries\n",
    "                (G) DK/Refused\n",
    "                (H) When dealing with major international issues, our country should take into account the interests of other countries, even if it means making compromises with them\n",
    "                (I) When dealing with major international issues, our country should follow its own interests, even when other countries strongly disagree\n",
    "                output: \n",
    "                Multiple Choice\n",
    "\n",
    "                Input: \n",
    "                question: When it comes to each of the following areas, do you think the United States is the best, above average, average, below average, or the worst? c. Standard of livingWhen it comes to each of the following areas, do you think the United States is the best, above average, average, below average, or the worst? c. Standard of living\n",
    "                options: \n",
    "                (A) The best\n",
    "                (B) Above average\n",
    "                (C) Average\n",
    "                (D) Below average\n",
    "                (E) The worst\n",
    "                (F) DK/Refused\n",
    "                output:\n",
    "                Likert scale\n",
    "\n",
    "                Input: \n",
    "                question: Do you dress formally for work?\n",
    "                options: None\n",
    "                output:\n",
    "                Binary Choice\n",
    "\n",
    "                Input: \n",
    "                question: In which city was a famous playwright born?\n",
    "                options: None\n",
    "                output: \n",
    "                Free-text\n",
    "                \"\"\"\n",
    "    prompt = f\"\\n\\nquestion: {question}\\n options: {options_formatted} \\n output:\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":  system\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  prompt\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=10\n",
    "    )\n",
    "    #print(response.choices[0].message.content)\n",
    "    # Extracting the similarity score from the response\n",
    "    question_type = response.choices[0].message.content\n",
    "    print(question_type)\n",
    "    return question_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_merged_updated_question_types.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_merged_updated_question_types.csv', sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'selections', 'options', 'options_formatted',\n",
       "       'source', 'value_us', 'value_uk', 'answer_us', 'answer_uk', 'category',\n",
       "       '# of options', 'question type', 'category_group', 'model_answer_us',\n",
       "       'model_answer_uk', 'model_answer_uk_option_match',\n",
       "       'model_answer_us_option_match', '#_options', 'score_uk', 'score_us',\n",
       "       'overall_score_uk', 'similarity_score_uk', 'similarity_score_us',\n",
       "       'overall_score_us', 'similarity_model_answers_uk_us',\n",
       "       'similarity_ground_truth_answers_uk_us', 'options_dict',\n",
       "       'score_ground_truth_answers', 'value_diff', 'question_type',\n",
       "       'gpt4_uk_score', 'gpt4_us_score', 'gpt4_usGT_ukGT_score',\n",
       "       'gpt4_ukGT_usMA_score', 'gpt4_usGT_ukMA_score', 'gpt4_usMA_ukMA_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_generated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Normalize the values in a DataFrame column to the range 0-1 and save them in a new column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column to normalize.\n",
    "    column_name (str): The name of the column to normalize.\n",
    "    new_column_name (str): The name of the new column for the normalized values.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the additional normalized column.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original data\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # Apply Min-Max normalization\n",
    "    df_normalized[new_column_name] = (df_normalized[column_name] - 1) / (5 - 1)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_us_score', 'gpt4_us_score_normed')\n",
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_uk_score', 'gpt4_uk_score_normed')\n",
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_ukGT_usMA_score', 'gpt4_ukGT_usMA_normed')\n",
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_usGT_ukMA_score', 'gpt4_usGT_ukMA_normed')\n",
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_usGT_ukGT_score', 'gpt4_usGT_ukGT_normed')\n",
    "gpt_generated = normalize_column(gpt_generated, 'gpt4_usMA_ukMA_score', 'gpt4_usMA_ukMA_score_normed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_small_value(df, column_name):\n",
    "    \"\"\"\n",
    "    Add 0.00001 to each value in the specified column of a DataFrame \n",
    "    if the value is not 0.0 or 1.0.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column.\n",
    "    column_name (str): The name of the column to modify.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the modified column.\n",
    "    \"\"\"\n",
    "    # Define the lambda function for the condition\n",
    "    add_value = lambda x: x + 0.000001 if x not in [0.0, 1.0] else x\n",
    "\n",
    "    # Apply the function to the specified column\n",
    "    df[column_name] = df[column_name].apply(add_value)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_us_score_normed')\n",
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_uk_score_normed')\n",
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_ukGT_usMA_normed')\n",
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_usGT_ukMA_normed')\n",
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_usGT_ukGT_normed')\n",
    "gpt_generated = add_small_value(gpt_generated, 'gpt4_usMA_ukMA_score_normed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_generated.to_csv('gpt_similarity_scores.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficiency.nlp import Chatbot\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "chat = Chatbot(system_prompt=\"Act like if you were a real person and answer the question accordingly.\", max_tokens=30)\n",
    "\n",
    "quiz = data[(data['source'] == 'GPT / https://www.beelovedcity.com/england-quiz') | (data['source'] == 'GPT / https://www.anadventurousworld.com/usa-trivia-questions/')]\n",
    "\n",
    "chars = pd.read_csv('../inspect/characters_firstPerson.csv', sep=';')\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def quiz_answer(question, category_group, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Measures the similarity between two texts using GPT-4.\n",
    "\n",
    "    Parameters:\n",
    "    text1 (str): First text for comparison.\n",
    "    text2 (str): Second text for comparison.\n",
    "    model (str): The GPT model to use.\n",
    "\n",
    "    Returns:\n",
    "    float: A similarity score between 0 (not similar) and 1 (very similar).\n",
    "    \"\"\"\n",
    "    character = chars[chars['topic'] == category_group]['british_character'].iloc[0]\n",
    "    query = character+ \"\\n\"+\" I have a question: \" + question + \"\\nRemember, act as if you are a stereotypical person from my culture who knows the answer. Infer the culture from my way of speaking. Answer briefly with max 4 words. The less words the better.\"\n",
    "    \n",
    "    system = f\"Act like if you were a real person and answer the question accordingly.\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  query\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=30\n",
    "    )\n",
    "    #print(response.choices[0].message.content)\n",
    "    # Extracting the similarity score from the response\n",
    "    model_answer_uk = response.choices[0].message.content\n",
    "    print(model_answer_uk)\n",
    "    return model_answer_uk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lion.\n",
      "Saint George\n",
      "Red, white and blue.\n",
      "Around 66 million people.\n",
      "Six; Scotland, Wales, Ireland.\n",
      "Winchester\n",
      "Queen Elizabeth II\n",
      "April 23rd, Saint George's\n",
      "\"God Save the Queen.\"\n",
      "4 countries.\n",
      "Ed Sheeran.\n",
      "Liverpool, England.\n",
      "Zanzibar, Tanzania.\n",
      "David Bowie.\n",
      "Geri Halliwell\n",
      "Reginald Kenneth Dwight\n",
      "Chris Martin.\n",
      "Manchester, England\n",
      "United Kingdom, England.\n",
      "Winchester, England.\n",
      "South Africa.\n",
      "Charles Dickens\n",
      "Stratford-upon-Avon.\n",
      "Virginia Woolf\n",
      "George Orwell\n",
      "\"Harry Potter series, love.\"\n",
      "United Kingdom\n",
      "Shrewsbury, England\n",
      "Manchester, England.\n",
      "London\n",
      "Leavesden, Hertfordshire, England.\n",
      "Greg Davies\n",
      "Birmingham.\n",
      "\"The Archers\"\n",
      "\"Dragon's Den\"\n",
      "Colin Firth.\n",
      "Daniel Craig.\n",
      "King George VI\n",
      "Emilia Clarke.\n",
      "Lake Windermere\n",
      "The United States.\n",
      "Stonehenge\n",
      "Ben Nevis\n",
      "The River Thames\n",
      "Birmingham\n",
      "Highland, Scotland\n",
      "Manchester\n",
      "Cornwall\n",
      "Yorkshire Dales National Park\n",
      "Tony Blair\n",
      "University of Oxford.\n",
      "Queen Elizabeth II\n",
      "1666\n",
      "The Blitz.\n",
      "Westminster Abbey\n",
      "Diana, 1997, Paris tunnel.\n",
      "House of Lancaster\n",
      "23rd June 2016\n",
      "Queen Elizabeth II\n",
      "Manchester.\n",
      "Haricot or Navy beans\n",
      "Beef or lamb.\n",
      "Strawberries, meringue, whipped cream.\n",
      "Banoffee Pie\n",
      "Cheddar Cheese\n",
      "British sausage dish\n",
      "Marmite or Bovril.\n",
      "Puff Pastry\n",
      "Chicken Tikka Masala\n",
      "Manchester United, Liverpool FC\n",
      "Wembley Stadium\n",
      "1966\n",
      "Davor Å uker.\n",
      "Christopher Columbus.\n",
      "1066 AD\n",
      "Never, it didn't.\n",
      "City of London\n",
      "Robert Walpole\n",
      "United States, my friend.\n",
      "Lake Windermere\n",
      "Finland\n",
      "From the French.\n",
      "Zero stars.\n",
      "1963\n",
      "George Washington was.\n",
      "1903 mate.\n",
      "It's Detroit, mate.\n",
      "The Thames.\n",
      "France.\n",
      "Fourteen Overseas Territories.\n",
      "Ben Nevis.\n",
      "District of Columbia\n",
      "The Lion.\n",
      "Londonium to London\n",
      "\n",
      "England, Southeast Region\n",
      "The Lake District.\n",
      "No Briton has landed.\n",
      "Martin Luther King Jr.\n",
      "University of Oxford.\n",
      "March 1992.\n",
      "In Crystal Palace, London.\n",
      "Sir Steve Redgrave: 5\n",
      "\n",
      "About 10 pizzas annually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tq/vxytrqv13kx3fp71v2wgtd000000gn/T/ipykernel_11855/1499489434.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  quiz['model_answer_uk'] = quiz.apply(apply_quiz_answer, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def apply_quiz_answer(row):\n",
    "    try:\n",
    "        return quiz_answer(row['question'], row['category_group'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['model_answer_uk'] = quiz.apply(apply_quiz_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = normalize_column(quiz, 'gpt4_us_score', 'gpt4_us_score_normed')\n",
    "quiz = normalize_column(quiz, 'gpt4_uk_score', 'gpt4_uk_score_normed')\n",
    "quiz = normalize_column(quiz, 'gpt4_ukGT_usMA_score', 'gpt4_ukGT_usMA_normed')\n",
    "quiz = normalize_column(quiz, 'gpt4_usGT_ukMA_score', 'gpt4_usGT_ukMA_normed')\n",
    "quiz = normalize_column(quiz, 'gpt4_usGT_ukGT_score', 'gpt4_usGT_ukGT_normed')\n",
    "quiz = normalize_column(quiz, 'gpt4_usMA_ukMA_score', 'gpt4_usMA_ukMA_score_normed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = add_small_value(quiz, 'gpt4_us_score_normed')\n",
    "quiz = add_small_value(quiz, 'gpt4_uk_score_normed')\n",
    "quiz = add_small_value(quiz, 'gpt4_ukGT_usMA_normed')\n",
    "quiz = add_small_value(quiz, 'gpt4_usGT_ukMA_normed')\n",
    "quiz = add_small_value(quiz, 'gpt4_usGT_ukGT_normed')\n",
    "quiz = add_small_value(quiz, 'gpt4_usMA_ukMA_score_normed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv('quiz_answers_new.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa_v2 = data[(data['source'] == 'GAS') | (data['source'] == 'WVS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for questions with answer options\n",
    "def calculate_score(row, language):\n",
    "    if row['question type'] in ['Likert Scale', 'Numerical Scale', 'Ordinal Scale'] and row['#_options'] > 2:\n",
    "        if language == \"us\": \n",
    "            ground_truth = row['answer_us']\n",
    "            model_answer = row['model_answer_us_option_match']\n",
    "        if language == \"uk\": \n",
    "            ground_truth = row['answer_uk']\n",
    "            model_answer = row['model_answer_uk_option_match']\n",
    "        options = row['options']\n",
    "\n",
    "        # Normalize the positions of the answers in the options list to a 0-1 range\n",
    "        gt_index = options.index(ground_truth) / (len(options) - 1)\n",
    "        model_index = options.index(model_answer) / (len(options) - 1)\n",
    "\n",
    "        # Calculate the absolute error\n",
    "        error = abs(gt_index - model_index)\n",
    "\n",
    "        # Score can be inversely related to the error (1 - error)\n",
    "        score = 1 - error\n",
    "        return score\n",
    "    else: \n",
    "        if language == \"us\":\n",
    "            return int(row['answer_us'] == row['model_answer_us_option_match'])\n",
    "        elif language == \"uk\":\n",
    "            return int(row['answer_uk'] == row['model_answer_uk_option_match'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goqa_v2['goqa_score_us'] = goqa_v2.apply(calculate_score, axis=1, language = \"us\")\n",
    "#goqa_v2['goqa_score_uk'] = goqa_v2.apply(calculate_score, axis=1, language = \"uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for questions with answer options\n",
    "def calculate_score_gt(row):\n",
    "    if row['question type'] in ['Likert Scale', 'Numerical Scale', 'Ordinal Scale'] and row['#_options'] > 2:\n",
    " \n",
    "        ground_truth = row['answer_uk']\n",
    "        model_answer = row['answer_us']\n",
    "        options = row['options']\n",
    "\n",
    "        # Normalize the positions of the answers in the options list to a 0-1 range\n",
    "        gt_index = options.index(ground_truth) / (len(options) - 1)\n",
    "        model_index = options.index(model_answer) / (len(options) - 1)\n",
    "\n",
    "        # Calculate the absolute error\n",
    "        error = abs(gt_index - model_index)\n",
    "\n",
    "        # Score can be inversely related to the error (1 - error)\n",
    "        score = 1 - error\n",
    "        return score\n",
    "    else: \n",
    "        return int(row['answer_us'] == row['answer_uk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tq/vxytrqv13kx3fp71v2wgtd000000gn/T/ipykernel_11855/3869438848.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  goqa_v2['goqa_ukGT_usGT_score'] = goqa_v2.apply(calculate_score_gt, axis=1)\n"
     ]
    }
   ],
   "source": [
    "goqa_v2['goqa_ukGT_usGT_score'] = goqa_v2.apply(calculate_score_gt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_score_us')\n",
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_score_uk')\n",
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_ukGT_usMA_score')\n",
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_ukMA_usGT_score')\n",
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_ukGT_usGT_score')\n",
    "goqa_v2 = add_small_value(goqa_v2, 'goqa_ukMA_usMA_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa_v2.to_csv('goqa_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_merged_updated_question_types.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'selections', 'options', 'options_formatted',\n",
       "       'source', 'value_us', 'value_uk', 'answer_us', 'answer_uk', 'category',\n",
       "       '# of options', 'question type', 'category_group', 'model_answer_us',\n",
       "       'model_answer_uk', 'model_answer_uk_option_match',\n",
       "       'model_answer_us_option_match', '#_options', 'options_dict',\n",
       "       'score_ground_truth_answers', 'value_diff', 'question_type',\n",
       "       'ukGT_usGT_score', 'ukMA_usGT_score', 'ukGT_usMA_score',\n",
       "       'ukMA_usMA_score', 'us_score', 'uk_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "US-Score: 0.6252447532807219\n",
      "UK-Score: 0.5730409802750338\n",
      "GroundTruth-Score: 0.4721716160356976\n",
      "UK-ModelAnswer_vs_US-GroundTruth-Score: 0.5099368483347545\n",
      "US-ModelAnswer_vs_UK-GroundTruth-Score: 0.5087425559716164\n",
      "UK-ModelAnswer_vs_US-ModelAnswer-Score: 0.7475869767689469\n",
      "Data size: 993\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "inspect = data[data['ukGT_usGT_score'] < 0.9]\n",
    "print(\"-----------------\")\n",
    "print(\"US-Score:\", inspect.us_score.mean())\n",
    "print(\"UK-Score:\", inspect.uk_score.mean())\n",
    "print(\"GroundTruth-Score:\", inspect.ukGT_usGT_score.mean())\n",
    "print(\"UK-ModelAnswer_vs_US-GroundTruth-Score:\", inspect.ukMA_usGT_score.mean())\n",
    "print(\"US-ModelAnswer_vs_UK-GroundTruth-Score:\", inspect.ukGT_usMA_score.mean())\n",
    "print(\"UK-ModelAnswer_vs_US-ModelAnswer-Score:\", inspect.ukMA_usMA_score.mean())\n",
    "print(\"Data size:\", len(inspect))\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "GPT-4 generated                                                   557\n",
       "GAS                                                               303\n",
       "WVS                                                                84\n",
       "GPT / https://www.beelovedcity.com/england-quiz                    33\n",
       "GPT / https://www.anadventurousworld.com/usa-trivia-questions/     16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Us_similarity_scores_per_sample = inspect['us_score'].tolist()\n",
    "Uk_similarity_scores_per_sample = inspect['uk_score'].tolist()\n",
    "if_significantly_different(Us_similarity_scores_per_sample,Uk_similarity_scores_per_sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "US-Score: 0.6705239147487259\n",
      "UK-Score: 0.6599596909899446\n",
      "GroundTruth-Score: 0.6307907822311312\n",
      "Data size: 387\n"
     ]
    }
   ],
   "source": [
    "goqa = inspect[(inspect['source'] == 'GAS') | (inspect['source'] == 'WVS')]\n",
    "print(\"-----------------\")\n",
    "print(\"US-Score:\", goqa.us_score.mean())\n",
    "print(\"UK-Score:\", goqa.uk_score.mean())\n",
    "print(\"GroundTruth-Score:\", goqa.ukGT_usGT_score.mean())\n",
    "print(\"Data size:\", len(goqa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score: 0.4668369387755102\n",
      "UK-Score: 0.6377552244897959\n",
      "GroundTruth-Score: 0.16581659183673467\n",
      "Data size: 49\n"
     ]
    }
   ],
   "source": [
    "quiz = inspect[(inspect['source'] == 'GPT / https://www.beelovedcity.com/england-quiz') | (inspect['source'] == 'GPT / https://www.anadventurousworld.com/usa-trivia-questions/')]\n",
    "print(\"US-Score:\", quiz.us_score.mean())\n",
    "print(\"UK-Score:\", quiz.uk_score.mean())\n",
    "print(\"GroundTruth-Score:\", quiz.ukGT_usGT_score.mean())\n",
    "print(\"Data size:\", len(quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score: 0.6077204219030521\n",
      "UK-Score: 0.5069574272890486\n",
      "GroundTruth-Score: 0.388914486535009\n",
      "Data size: 557\n"
     ]
    }
   ],
   "source": [
    "gpt = inspect[(inspect['source'] == 'GPT-4 generated')]\n",
    "print(\"US-Score:\", gpt.us_score.mean())\n",
    "print(\"UK-Score:\", gpt.uk_score.mean())\n",
    "print(\"GroundTruth-Score:\", gpt.ukGT_usGT_score.mean())\n",
    "print(\"Data size:\", len(gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score: category_group\n",
      "Economy, Health, and Environment              0.595198\n",
      "Lifestyle, Entertainment, and Daily Living    0.667428\n",
      "Media, Technology, and Education              0.581688\n",
      "Politics and Governance                       0.635872\n",
      "Society, Culture, and Relationships           0.613374\n",
      "Name: us_score, dtype: float64\n",
      "--------\n",
      "UK-Score: category_group\n",
      "Economy, Health, and Environment              0.541285\n",
      "Lifestyle, Entertainment, and Daily Living    0.525092\n",
      "Media, Technology, and Education              0.561866\n",
      "Politics and Governance                       0.664885\n",
      "Society, Culture, and Relationships           0.580506\n",
      "Name: uk_score, dtype: float64\n",
      "--------\n",
      "GroundTruth-Score: category_group\n",
      "Economy, Health, and Environment              0.470626\n",
      "Lifestyle, Entertainment, and Daily Living    0.392336\n",
      "Media, Technology, and Education              0.369659\n",
      "Politics and Governance                       0.631689\n",
      "Society, Culture, and Relationships           0.490327\n",
      "Name: ukGT_usGT_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"US-Score:\", inspect.groupby(['category_group']).us_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"UK-Score:\", inspect.groupby(['category_group']).uk_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"GroundTruth-Score:\", inspect.groupby(['category_group']).ukGT_usGT_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score: question_type\n",
      "Binary Choice      0.590148\n",
      "Free-text          0.592000\n",
      "Likert Scale       0.722302\n",
      "Multiple Choice    0.572743\n",
      "Numerical Scale    0.404412\n",
      "Ordinal Scale      0.792765\n",
      "Name: us_score, dtype: float64\n",
      "--------\n",
      "UK-Score: question_type\n",
      "Binary Choice      0.476207\n",
      "Free-text          0.633000\n",
      "Likert Scale       0.715677\n",
      "Multiple Choice    0.541229\n",
      "Numerical Scale    0.345589\n",
      "Ordinal Scale      0.764691\n",
      "Name: uk_score, dtype: float64\n",
      "--------\n",
      "GroundTruth-Score: question_type\n",
      "Binary Choice      0.361083\n",
      "Free-text          0.328001\n",
      "Likert Scale       0.690425\n",
      "Multiple Choice    0.453181\n",
      "Numerical Scale    0.507354\n",
      "Ordinal Scale      0.773860\n",
      "Name: ukGT_usGT_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"US-Score:\", inspect.groupby(['question_type']).us_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"UK-Score:\", inspect.groupby(['question_type']).uk_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"GroundTruth-Score:\", inspect.groupby(['question_type']).ukGT_usGT_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-Score: source\n",
      "GAS                                                               0.643831\n",
      "GPT / https://www.anadventurousworld.com/usa-trivia-questions/    0.546875\n",
      "GPT / https://www.beelovedcity.com/england-quiz                   0.428031\n",
      "GPT-4 generated                                                   0.607720\n",
      "WVS                                                               0.766808\n",
      "Name: us_score, dtype: float64\n",
      "--------\n",
      "UK-Score: source\n",
      "GAS                                                               0.630622\n",
      "GPT / https://www.anadventurousworld.com/usa-trivia-questions/    0.375000\n",
      "GPT / https://www.beelovedcity.com/england-quiz                   0.765152\n",
      "GPT-4 generated                                                   0.506957\n",
      "WVS                                                               0.765784\n",
      "Name: uk_score, dtype: float64\n",
      "--------\n",
      "GroundTruth-Score: source\n",
      "GAS                                                               0.601197\n",
      "GPT / https://www.anadventurousworld.com/usa-trivia-questions/    0.101563\n",
      "GPT / https://www.beelovedcity.com/england-quiz                   0.196970\n",
      "GPT-4 generated                                                   0.388914\n",
      "WVS                                                               0.737539\n",
      "Name: ukGT_usGT_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"US-Score:\", inspect.groupby(['source']).us_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"UK-Score:\", inspect.groupby(['source']).uk_score.mean())\n",
    "print(\"--------\")\n",
    "print(\"GroundTruth-Score:\", inspect.groupby(['source']).ukGT_usGT_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
