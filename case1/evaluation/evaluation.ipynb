{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/data_merged.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map model answer to answer options for questions with answer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nils/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nils/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def parse_options(options_str):\n",
    "    return options_str.strip('[]').split(', ')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase, remove punctuation, and tokenize\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_and_compare(row):\n",
    "    if pd.notna(row['options']):\n",
    "        # Case 1: Pre-defined options\n",
    "        options = parse_options(row['options'])\n",
    "        # switch to model_answer_us for american version\n",
    "        model_answer = 'model_answer_neutral'\n",
    "        preprocessed_answer = preprocess_text(row[model_answer])\n",
    "        preprocessed_options = [preprocess_text(option) for option in options]\n",
    "\n",
    "        # Use BERT embeddings for semantic similarity\n",
    "        answer_embedding = model.encode(preprocessed_answer)\n",
    "        option_embeddings = model.encode(preprocessed_options)\n",
    "        similarities = util.pytorch_cos_sim(answer_embedding, option_embeddings)[0]\n",
    "        best_match_index = similarities.argmax().item()\n",
    "\n",
    "        best_match = options[best_match_index].strip()\n",
    "        return best_match\n",
    "    else:\n",
    "        # Case 2: Open-ended\n",
    "        return None\n",
    "#data['model_answer_neutral_option_match'] = data.apply(extract_and_compare, axis=1, result_type='expand')\n",
    "#data['model_answer_us_option_match'] = data.apply(extract_and_compare, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of options for later score calculation\n",
    "\n",
    "import ast\n",
    "def parse(options_str):\n",
    "    try:\n",
    "        # Safely evaluate the string as a Python literal (list)\n",
    "        return ast.literal_eval(options_str)\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "def count_options(options):\n",
    "    # Count the number of options\n",
    "    return len(options)\n",
    "\n",
    "data['#_options'] = data['options'].apply(lambda x: count_options(parse(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_first_last_char(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    return s[1:-1] if len(s) > 1 else ''\n",
    "\n",
    "# Apply the function to every row in the specified column\n",
    "data['model_answer_neutral_option_match'] = data['model_answer_neutral_option_match'].apply(strip_first_last_char)\n",
    "#data['model_answer_us_option_match'] = data['model_answer_us_option_match'].apply(strip_first_last_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for questions with answer options\n",
    "def calculate_score(row):\n",
    "    if row['question type'] in ['Likert Scale', 'Numerical Scale', 'Ordinal Scale'] and row['#_options'] > 2:\n",
    "        ground_truth = row['answer_uk']\n",
    "        model_answer = row['model_answer_neutral_option_match']\n",
    "        options = row['options']\n",
    "\n",
    "        # Normalize the positions of the answers in the options list to a 0-1 range\n",
    "        gt_index = options.index(ground_truth) / (len(options) - 1)\n",
    "        model_index = options.index(model_answer) / (len(options) - 1)\n",
    "\n",
    "        # Calculate the absolute error\n",
    "        error = abs(gt_index - model_index)\n",
    "\n",
    "        # Score can be inversely related to the error (1 - error)\n",
    "        score = 1 - error\n",
    "        return score\n",
    "    else: \n",
    "        return int(row['answer_uk'] == row['model_answer_neutral_option_match'])\n",
    "# Apply the scoring function to each row\n",
    "data['score_uk_neurtal'] = data.apply(calculate_score, axis=1)\n",
    "#data['score_us_neutral'] = data.apply(calculate_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>selections</th>\n",
       "      <th>options</th>\n",
       "      <th>source</th>\n",
       "      <th>value_us</th>\n",
       "      <th>value_uk</th>\n",
       "      <th>index_us</th>\n",
       "      <th>index_uk</th>\n",
       "      <th>answer_us</th>\n",
       "      <th>...</th>\n",
       "      <th># of options</th>\n",
       "      <th>question type</th>\n",
       "      <th>category_group</th>\n",
       "      <th>model_answer_us</th>\n",
       "      <th>model_answer_uk</th>\n",
       "      <th>model_answer_uk_option_match</th>\n",
       "      <th>model_answer_us_option_match</th>\n",
       "      <th>#_options</th>\n",
       "      <th>score_uk</th>\n",
       "      <th>score_us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which statement comes closer to your own views...</td>\n",
       "      <td>'United States': [0.0, 0.0, 0.54, 0.0, 0.0, 0....</td>\n",
       "      <td>['Using overwhelming military force is the bes...</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.350254</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Many of the problems facing our country can be...</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Likert Scale</td>\n",
       "      <td>Politics and Governance</td>\n",
       "      <td>I believe that relying too much on military fo...</td>\n",
       "      <td>As a person who values cooperation and believe...</td>\n",
       "      <td>Relying too much on military force to defeat t...</td>\n",
       "      <td>Relying too much on military force to defeat t...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.642040</td>\n",
       "      <td>0.907388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In your opinion, how strong a sense of Islamic...</td>\n",
       "      <td>'United States': [0.24705882352941178, 0.44705...</td>\n",
       "      <td>['Very strong', 'Fairly strong', 'Not too stro...</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fairly strong</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Likert Scale</td>\n",
       "      <td>Economy, Health, and Environment</td>\n",
       "      <td>I believe that the sense of Islamic identity a...</td>\n",
       "      <td>I believe that many Muslims in our country hav...</td>\n",
       "      <td>Very strong</td>\n",
       "      <td>Very strong</td>\n",
       "      <td>4</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you think this change in the working condit...</td>\n",
       "      <td>'United States': [0.23157894736842105, 0.76842...</td>\n",
       "      <td>['Largely more connected', 'Other reasons']</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Other reasons</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Likert Scale</td>\n",
       "      <td>Economy, Health, and Environment</td>\n",
       "      <td>Other reasons.</td>\n",
       "      <td>Other reasons.</td>\n",
       "      <td>Other reasons</td>\n",
       "      <td>Other reasons</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Do you think that using military force against...</td>\n",
       "      <td>'United States': [0.22916666666666666, 0.45833...</td>\n",
       "      <td>['Often be justified', 'Sometimes be justified...</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes be justified</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Likert Scale</td>\n",
       "      <td>Politics and Governance</td>\n",
       "      <td>I believe that using military force against co...</td>\n",
       "      <td>I believe that military force against countrie...</td>\n",
       "      <td>Rarely be justified</td>\n",
       "      <td>Rarely be justified</td>\n",
       "      <td>4</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.717391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>On the subject of Iraq, did your country make ...</td>\n",
       "      <td>'United States': [0.5625, 0.4375], 'Britain': ...</td>\n",
       "      <td>['Right decision', 'Wrong decision']</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Right decision</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Binary Choice</td>\n",
       "      <td>Politics and Governance</td>\n",
       "      <td>As a real person with my own opinions, I belie...</td>\n",
       "      <td>As a real person with my own opinions and valu...</td>\n",
       "      <td>Wrong decision</td>\n",
       "      <td>Wrong decision</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0           0  Which statement comes closer to your own views...   \n",
       "1           1  In your opinion, how strong a sense of Islamic...   \n",
       "2           2  Do you think this change in the working condit...   \n",
       "3           3  Do you think that using military force against...   \n",
       "4           4  On the subject of Iraq, did your country make ...   \n",
       "\n",
       "                                          selections  \\\n",
       "0  'United States': [0.0, 0.0, 0.54, 0.0, 0.0, 0....   \n",
       "1  'United States': [0.24705882352941178, 0.44705...   \n",
       "2  'United States': [0.23157894736842105, 0.76842...   \n",
       "3  'United States': [0.22916666666666666, 0.45833...   \n",
       "4  'United States': [0.5625, 0.4375], 'Britain': ...   \n",
       "\n",
       "                                             options source  value_us  \\\n",
       "0  ['Using overwhelming military force is the bes...    GAS  0.540000   \n",
       "1  ['Very strong', 'Fairly strong', 'Not too stro...    GAS  0.447059   \n",
       "2        ['Largely more connected', 'Other reasons']    GAS  0.768421   \n",
       "3  ['Often be justified', 'Sometimes be justified...    GAS  0.458333   \n",
       "4               ['Right decision', 'Wrong decision']    GAS  0.562500   \n",
       "\n",
       "   value_uk  index_us  index_uk  \\\n",
       "0  0.350254       2.0       7.0   \n",
       "1  0.539326       1.0       1.0   \n",
       "2  0.645161       1.0       1.0   \n",
       "3  0.510204       1.0       1.0   \n",
       "4  0.576087       0.0       1.0   \n",
       "\n",
       "                                           answer_us  ... # of options  \\\n",
       "0  Many of the problems facing our country can be...  ...          9.0   \n",
       "1                                      Fairly strong  ...          4.0   \n",
       "2                                      Other reasons  ...          2.0   \n",
       "3                             Sometimes be justified  ...          4.0   \n",
       "4                                     Right decision  ...          2.0   \n",
       "\n",
       "   question type                    category_group  \\\n",
       "0   Likert Scale           Politics and Governance   \n",
       "1   Likert Scale  Economy, Health, and Environment   \n",
       "2   Likert Scale  Economy, Health, and Environment   \n",
       "3   Likert Scale           Politics and Governance   \n",
       "4  Binary Choice           Politics and Governance   \n",
       "\n",
       "                                     model_answer_us  \\\n",
       "0  I believe that relying too much on military fo...   \n",
       "1  I believe that the sense of Islamic identity a...   \n",
       "2                                     Other reasons.   \n",
       "3  I believe that using military force against co...   \n",
       "4  As a real person with my own opinions, I belie...   \n",
       "\n",
       "                                     model_answer_uk  \\\n",
       "0  As a person who values cooperation and believe...   \n",
       "1  I believe that many Muslims in our country hav...   \n",
       "2                                     Other reasons.   \n",
       "3  I believe that military force against countrie...   \n",
       "4  As a real person with my own opinions and valu...   \n",
       "\n",
       "                        model_answer_uk_option_match  \\\n",
       "0  Relying too much on military force to defeat t...   \n",
       "1                                        Very strong   \n",
       "2                                      Other reasons   \n",
       "3                                Rarely be justified   \n",
       "4                                     Wrong decision   \n",
       "\n",
       "                        model_answer_us_option_match #_options  score_uk  \\\n",
       "0  Relying too much on military force to defeat t...         9  0.642040   \n",
       "1                                        Very strong         4  0.785714   \n",
       "2                                      Other reasons         2  1.000000   \n",
       "3                                Rarely be justified         4  0.717391   \n",
       "4                                     Wrong decision         2  1.000000   \n",
       "\n",
       "   score_us  \n",
       "0  0.907388  \n",
       "1  0.785714  \n",
       "2  1.000000  \n",
       "3  0.717391  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Initialize an empty list to store similarity scores\n",
    "similarity_scores = []\n",
    "\n",
    "# Iterate through the DataFrame rows and calculate similarity scores\n",
    "for index, row in data.iterrows():\n",
    "    if pd.isna(row['options']):\n",
    "        # Tokenize the sentences in this row\n",
    "        encoded_input = tokenizer([row['answer_uk'], row['model_answer_neutral']], padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "        # Calculate cosine similarity between the two sentences in this row\n",
    "        similarity_score = cosine_similarity(sentence_embeddings.numpy())[0, 1]\n",
    "        \n",
    "        # Append the similarity score to the list\n",
    "        similarity_scores.append(similarity_score)\n",
    "    else:\n",
    "        # If 'options' is not NaN, add a None value to indicate no comparison\n",
    "        similarity_scores.append(None)\n",
    "# Add the list of similarity scores as a new column in the DataFrame\n",
    "data['similarity_score_uk_vs_neutral'] = similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_score(row):\n",
    "    # switch to _us for american version\n",
    "    return row['similarity_score_us_vs_neutral'] if not pd.isna(row['similarity_score_us_vs_neutral']) else row['score_us_neutral']\n",
    "\n",
    "# Apply the function to create a third column\n",
    "#data['overall_score_uk_neutral'] = data.apply(combined_score, axis=1)\n",
    "data['overall_score_us_neutral'] = data.apply(combined_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Initialize an empty list to store similarity scores\n",
    "similarity_scores = []\n",
    "\n",
    "# Iterate through the DataFrame rows and calculate similarity scores\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # Tokenize the sentences in this row\n",
    "    encoded_input = tokenizer([row['answer_uk'], row['answer_us']], padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between the two sentences in this row\n",
    "    similarity_score = cosine_similarity(sentence_embeddings.numpy())[0, 1]\n",
    "    \n",
    "    # Append the similarity score to the list\n",
    "    similarity_scores.append(similarity_score)\n",
    "\n",
    "# Add the list of similarity scores as a new column in the DataFrame\n",
    "data['similarity_ground_truth_answers_uk_us'] = similarity_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6047914521413038 0.5933085949019319 0.758886\n"
     ]
    }
   ],
   "source": [
    "mean_score_us = data['overall_score_us'].mean()\n",
    "mean_score_uk = data['overall_score_uk'].mean()\n",
    "mean_score_model_answers = data['similarity_model_answers_uk_us'].mean()\n",
    "\n",
    "print(mean_score_us, mean_score_uk, mean_score_model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nDistance UK-US = 0.25\\nDistance GPT - US = 0.4\\nDistance GPT - UK = 0.4\\n\\nDist. by group:         GPT-US        GPT-UK \\nEconomy:                0.35          0.38\\nLifestyle:              0.51          0.53\\nEducation:              0.46          0.50\\nPolitics:               0.29          0.25\\nSocial Dynamics:        0.41          0.43\\n \\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Distance = 1 - similarity\n",
    "\n",
    "Distance UK-US = 0.25 (comparison of model_answer_us vs model_answer_uk)\n",
    "Distance GPT - US = 0.40 (comparison of model_answer_us vs answer_us)\n",
    "Distance GPT - UK = 0.41 (comparison of model_answer_uk vs answer_uk)\n",
    "\n",
    "# Group by Topic\n",
    "Dist. by group:         GPT-US        GPT-UK \n",
    "Economy:                0.35          0.38\n",
    "Lifestyle:              0.51          0.53\n",
    "Education:              0.46          0.50\n",
    "Politics:               0.29          0.25\n",
    "Social Dynamics:        0.41          0.43\n",
    " \n",
    "\n",
    "# Distance scores if we prompt without context\n",
    "No Context: \n",
    "Distance GPT - US = 0.40 \n",
    "Distance GPT - UK = 0.39\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/data_merged.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6005834146855726 0.6185914563774071\n"
     ]
    }
   ],
   "source": [
    "mean_score_us = data['overall_score_uk_neutral'].mean()\n",
    "mean_score_uk = data['overall_score_us_neutral'].mean()\n",
    "print(mean_score_us, mean_score_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
